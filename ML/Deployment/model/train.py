# -*- coding: utf-8 -*-
"""Anarki.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Kpt3uy7AxOOHa7J1SL2mDWpqEGxl8QG

# **Anarki : First Layer Detection of Drug Abuse by Face Recognition Based on Mobile Apps**

### **Load Dataset**
"""

import numpy as np
import os
import tensorflow as tf
import pathlib
import matplotlib.image as mpimg
import matplotlib.pyplot as plt

# Check TF 2 Version
tf.__version__

# Download Dataset 
dataset_url = "https://raw.githubusercontent.com/amaripujo/drug_abused/main/Update/drug_abused.tar.gz"
data_dir = tf.keras.utils.get_file(origin=dataset_url, fname='drug_abused', untar=True)
data_dir = pathlib.Path(data_dir)

# Mengecek Directori Dataset
print(data_dir)

# Direktori training gambar before dan after
train_before_dir = os.path.join(data_dir, 'before')
train_after_dir = os.path.join(data_dir, 'after')

# Menampilkan daftar nama file masing-masing direktori
before_filenames = os.listdir(train_before_dir)
after_filenames = os.listdir(train_after_dir)

print(before_filenames[:10])
print(after_filenames[:10])

# Melihat total gambar dalam direktori
print('Total Pelatihan gambar before :', len(os.listdir(train_before_dir)))
print('Total Pelatihan gambar after :', len(os.listdir(train_after_dir) ))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# Parameter dalam grafik yang akan menampilkan gambar 4x4
rows = 4
cols = 4

# Index untuk pengulangan gambar
indexes = 0

# Visualisasi gambar sebanyak 4x4, 8 gambar atas untuk before, 8 gambar bawah untuk after
fig = plt.gcf()
fig.set_size_inches(cols*4, rows*4)

indexes+=8

before_picture = [os.path.join(train_before_dir, fname) for fname in before_filenames[indexes-8:indexes]]

after_picture = [os.path.join(train_after_dir, fname) for fname in after_filenames[indexes-8:indexes]]

for i, img_path in enumerate(before_picture+after_picture):
  vis = plt.subplot(rows, cols, i + 1)
  vis.axis('off')

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""### **Preprocessing Data**"""

# Preprocessing data
train_data = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(180, 180),
  batch_size=32)

validation_data = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(180, 180),
  batch_size=32)

classes = train_data.class_names
print(classes)

for image_batch, labels_batch in train_data:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

"""#### **Standarisasi Data**"""

from tensorflow.keras import layers

normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)

normalized_data = train_data.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_data))
first_image = image_batch[0]
# Notice the pixels values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

"""#### **Konfigurasi Performa Dataset**"""

AUTOTUNE = tf.data.AUTOTUNE

train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
validation_data = validation_data.cache().prefetch(buffer_size=AUTOTUNE)

"""### **Build the Model**"""

DESIRED_ACCURACY = 0.99

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>DESIRED_ACCURACY):
      print('\nReached 99% accuracy so cancelling training!')
      self.model.stop_training = True
    

callbacks = myCallback()

# Data Augmentation untuk Menghindari Overfitting
data_augmentation = tf.keras.Sequential(
  [
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal", input_shape=(180, 180,3)),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),
  ]
)

# Membangun Model
from tensorflow.keras.optimizers import RMSprop

model = tf.keras.models.Sequential([
    data_augmentation,
    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),
    tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation ='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dense(1, activation ='sigmoid')
])

model.compile(loss='binary_crossentropy', 
              optimizer=RMSprop(learning_rate=0.001), 
              metrics=['acc'])
model.summary()

"""### **Training Data**"""

history = model.fit(
      train_data, 
      epochs=100,
      steps_per_epoch=10,
      validation_data=validation_data, 
      callbacks=[callbacks])

"""### **Testing Data**"""

import numpy as np
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = '/content/' + fn
  img = image.load_img(path, target_size=(180, 180))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)
  print(classes[0])
  if classes[0]>0.5:
    print(fn + " is adicted")
  else:
    print(fn + " is not adicted")

#model.save
#joblib.dump(model, 'model.joblib')
#await model.save('downloads://my-model');

# Save model architecture to model.json

model_json = model.to_json()

with open('model.json','w') as file:
    file.write(model_json)


# Save model weights to model.h5

model.save_weights('model.h5')

print('Saved model')

"""### **Evaluating Accuracy and Loss Model**"""

# Visualisasi Hasil Accuracy dan Loss dalam Bentuk Grafik
acc      = history.history['acc']
val_acc  = history.history['val_acc']
loss     = history.history['loss']
val_loss = history.history['val_loss']

epochs   = range(len(acc))

# Grafik Akurasi Training dan Validation
plt.plot  (epochs, acc, label='Training Accuracy')
plt.plot  (epochs, val_acc, label= 'Validation Accuracy')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.title ('Training and Validation Accuracy')
plt.figure()

# Grafik Loss Training dan Validation
plt.plot  ( epochs, loss, label='Training Loss')
plt.plot  ( epochs, val_loss, label='Validation Loss')
plt.legend(['Training Loss', 'Validation Loss'])
plt.title ('Training and Validation Loss')
